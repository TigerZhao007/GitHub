---
title: "多头数据分析模型"
author: "sdk"
date: "2017年12月1日"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 数据读取

```{r warning=FALSE}
dat  <- read.csv('C:\\Users\\Administrator\\Desktop\\a.csv',header = T)
dat = dat[which(dat$result==1),]  
for (i in 7:17) {
  dat[,i] <- as.numeric(dat[,i])
}
dat1 <- dat[,c(8,11,12,13,14,15,16,17)]
dat2 <- dat[,c(7,11,12,13,14,15,16,17)]
# 注：1.读取数据，2.剔除其中没有反馈结果的四个电话号码，3.修改数据类型
```

## 数据透析函数定义

```{r warning=FALSE}
# 两分类数据透析函数
ks_pc <- function(pre,class,low=0,high=1,by=0.05){
  pred0 <- pre[which(class==0)]
  pred1 <- pre[which(class==1)]
  cut_0 <- cut(pred0, breaks = seq(low,high,by))
  cut_1 <- cut(pred1, breaks = seq(low,high,by))
  ks <- data.frame()
  ks[1:length(levels(cut_0)),1] <- levels(cut_0)
  for(i in 1:length(levels(cut_0))){  ks[i,2] <- table(cut_0)[[i]]  }
  for(i in 1:length(levels(cut_0))){  ks[i,3] <- table(cut_1)[[i]]  }
  ks[,4] <- cumsum(table(cut_0))/sum(ks$V2)
  ks[,5] <- cumsum(table(cut_1))/sum(ks$V3)
  ks[,6] <- ks[,4] - ks[,5]
  colnames(ks) <- c('section','bad_0','good_1','add_bad_0','add_good_1','ks_value')
  return(ks)
}
```

## 选择分析样本

```{r warning=FALSE }
# 确定分析样本
data <- dat1
# 查看分析样本
head(data)
# 查看样本分布情况
table(data$class)
```

## 样本等量化处理

```{r warning=FALSE }
# 利用ROSE包将样本进行等量化处理。
library(ROSE)
df <- ROSE(class~.,data = data,seed=1)$data
# 查看等量化处理后数据
head(df)
# 查看样本分布情况
table(df$class)
```

## 数据归一化处理

```{r warning=FALSE }
# 样本数据规约到0-1之间
for(i in 1:ncol(df)){
  df[,i] = (df[,i]-min(df[,i]))/(max(df[,i])-min(df[,i]))
}
# 查看归一化后分析数据
head(df)
```

## 样本随机化打散

```{r warning=FALSE }
# 通过随机数，随机将样本打散。
set.seed(1234)
df_sample <- sample(1:nrow(df),size = nrow(df),replace = F)
df <- df[df_sample,]
# 查看打散后分析数据
head(df)
```

## 确定训练样本和测试样本

```{r warning=FALSE }
# 通过随机数，随机将样本分区，其中70%为训练样本，30%为测试样本。
set.seed(1234)
train_sample <- sample(1:nrow(df),size = 0.7*nrow(df),replace = F)
df_train <- df[train_sample, ]
df_test  <- df[-train_sample, ]
```

## 逻辑回归

```{r warning=FALSE }
model_logic <- glm(class~., data = df_train, family = binomial (link = logit))
pred_logic <- predict(model_logic, newdata = df_test,type = "response")
ks_logic <- ks_pc(pred_logic,df_test$class)
ks_logic
```

## BP神经网络

```{r warning=FALSE }
library(nnet)
size = 5;   decay = 0.05;  maxit = 10000;
model_nnet <- nnet(class~., data = df_train,size=size,decay=decay,maxit=maxit)
pred_nnet <- predict(model_nnet, newdata = df_test)
ks_nnet <- ks_pc(pred_nnet,df_test$class)
ks_nnet
```

## 决策树

```{r warning=FALSE }
library(rpart)
library(rpart.plot)
ct <- rpart.control(xval=10, minsplit=20, cp=0.1)
model_tree <-  rpart(class~.,  data = df_train, method="anova", control=ct,
                     parms = list(prior = c(0.65,0.35), split = "information"))
pred_tree <- predict(model_tree,newdata = df_test)
ks_tree <- ks_pc(pred_tree,df_test$class)
ks_tree
```

## 随机森林

```{r warning=FALSE }
library(randomForest)
library(foreign)
model_randomforest <- randomForest(class~., data=df_train, mtry=2,
                                   ntree=10, importance = TRUE)
pred_randomforest <- predict(model_randomforest, newdata = df_test)
ks_randomforest <- ks_pc(pred_randomforest, df_test$class)
ks_randomforest
```

## KNN算法

```{r warning=FALSE }
library(kknn) 
model_kknn <- kknn(class~., train = df_train, test = df_test, k = 5) 
pred_kknn <- fitted(model_kknn)
ks_kknn <- ks_pc(pred_kknn, df_test$class)
ks_kknn
```

## KS值汇总

```{r warning=FALSE }
ks_list <- list(ks_logic=ks_logic, ks_nnet=ks_nnet, ks_tree=ks_tree, 
                ks_randomforest=ks_randomforest, ks_kknn=ks_kknn)
ks_list
```




