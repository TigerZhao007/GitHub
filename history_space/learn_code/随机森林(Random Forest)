# 随机森林(Random Forest)

## 随机森林的概念
随机森林顾名思义，是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。
在得到森林之后，当有一个新的输 入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），
然后看看哪一类被选择最多，就预测这个样本 为那一类。

在建立每一棵决策树的过程中，有两点需要注意 - 采样与完全分裂。首先是两个随机采样的过程，random forest对输入的数据要进行行、列的采样。
对于行采样，采用有放回的方式，也就是在采样得到的样本集合中，可能有重复的样本。假设输入样本为N个，那 么采样的样本也为N个。
这样使得在训练的时候，每一棵树的输入样本都不是全部的样本，使得相对不容易出现over-fitting。然后进行列采样，从M 个feature中，选择m个(m << M)。
之后就是对采样之后的数据使用完全分裂的方式建立出决策树，这样决策树的某一个叶子节点要么是无法继续分裂的，要么里面的所有样本的都是指向的同一 个分类。
一般很多的决策树算法都一个重要的步骤 - 剪枝，但是这里不这样干，由于之前的两个随机采样的过程保证了随机性，所以就算不剪枝，也不会出现over-fitting。

按这种算法得到的随机森林中的每一棵都是很弱的，但是大家组合起来就很厉害了。我觉得可以这样比喻随机森林算法：
每一棵决策树就是一个精通于某一个窄领域 的专家（因为我们从M个feature中选择m让每一棵决策树进行学习），
这样在随机森林中就有了很多个精通不同领域的专家，对一个新的问题（新的输入数据），可以用不同的角度去看待它，最终由各个专家，投票得到结果。

## 随机森林的优点
在数据集上表现良好
在当前的很多数据集上，相对其他算法有着很大的优势
它能够处理很高维度（feature很多）的数据，并且不用做特征选择
在训练完后，它能够给出哪些feature比较重要
在创建随机森林的时候，对generlization error使用的是无偏估计
训练速度快
在训练过程中，能够检测到feature间的互相影响
容易做成并行化方法
实现比较简单

## 随机森林的实现
随机森林在bagging的基础上更进一步：
1.  样本的随机：从样本集中用Bootstrap随机选取n个样本
2.  特征的随机：从所有属性中随机选取K个属性，选择最佳分割属性作为节点建立CART决策树（泛化的理解，这里面也可以是其他类型的分类器，比如SVM、Logistics）
3.  重复以上两步m次，即建立了m棵CART决策树
4.  这m个CART形成随机森林，通过投票表决结果，决定数据属于哪一类（投票机制有一票否决制、少数服从多数、加权多数）
关于调参：
1.如何选取K，可以考虑有N个属性，取K=根号N， 2.最大深度（不超过8层）
3.棵数               4.最小分裂样本树               5.类别比例
决策树的重要参数都是防止过拟合的. 有2个参数是关键，min_samples_leaf 这个sklearn的默认值是1，经验上必须大于100，
如果一个节点都没有100个样本支持他的决策，一般都被认为是过拟合；max_depth 这个参数控制树的规模。决策树是一个非常直观的机器学习方法。
一般我们都会把它的决策树结构打印出来观察，如果深度太深对于我们的理解是有难度的。



