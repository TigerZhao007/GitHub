# K-近邻算法（KNN）
================================================================================

##一、KNN算法概述
================================================================================
给一个训练数据集和一个新的实例，在训练数据集中找出与这个新实例最近的k个训练实例，然后统计最近的k个训练实例中所属类别技术最多的那个类，就是新实例的类。
KNN算法中，所选择的邻居都是已经正确分类的对象。该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。
KNN 算法本身简单有效，它是一种 lazy-learning 算法，分类器不需要使用训练集进行训练，训练时间复杂度为0。
KNN 分类的计算复杂度和训练集中的文档数目成正比，也就是说，如果训练集中文档总数为 n，那么 KNN 的分类时间复杂度为O(n)。
KNN既可以用于分类，也可以用于回归。
积极学习方法（eager learner）：决策树和基于规则的分类器，因为一旦训练数据可用，他们就开始学习从输入属性到类标号的映射模型。
消极学习法（lazy learner）：一个相反的策略是推迟对训练数据的建模，直到需要分类测试样例时再进行。

##二、KNN算法思想
================================================================================
KNN算法的思想总结一下：就是在训练集中数据和标签已知的情况下，输入测试数据，将测试数据的特征与训练集中对应的特征进行相互比较，
找到训练集中与之最为相似的前K个数据，则该测试数据对应的类别就是K个数据中出现次数最多的那个分类，其算法的描述为：
*计算测试数据与各个训练数据之间的距离；
*按照距离的递增关系进行排序；
*选取距离最小的K个点；
*确定前K个点所在类别的出现频率；
*返回前K个点中出现频率最高的类别作为测试数据的预测分类。

##三、KNN算法三要素
================================================================================
1、k值的选择（非常重要）
*K值的重要性：k值对模型的预测有着直接的影响
a.如果k值过小，预测结果对邻近的实例点非常敏感。如果邻近的实例恰巧是噪声数据，预测就会出错。
也就是说，k值越小就意味着整个模型就变得越复杂，越容易发生过拟合。
b.如果k值越大，有点是可以减少模型的预测误差，缺点是学习的近似误差会增大。会使得距离实例点较远的点也起作用，致使预测发生错误。
同时，k值的增大意味着模型变得越来越简单。如果k=N，那么无论输入实例是什么，都将简单的把它预测为样本中最多的一类。这显然实不可取的。
c.在实际建模应用中，k值一般取一个较小的数值，通常采用cross-validation的方法来选择最优的k值。
*距离的度量（常见的距离度量有欧式距离，马氏距离，夹角余弦等）
*分类决策规则（多数表决规则）
2、k值的选择：
*k值越小表明模型越复杂，更加容易过拟合
*但是k值越大，模型越简单，如果k=N的时候就表明无论什么点都是训练集中类别最多的那个类。 所以一般k会取一个较小的值，然后用交叉验证来确定（关于交叉验证)

##四、KNN算法的回归
================================================================================
在找到最近的k个实例之后，可以计算着k个实例的平均值作为预测值。或者还可以给这k个实例添加一个权重再求平均值，这个权重与度量距离成反比（越近权重越大）。

##五、KNN算法的优缺点
================================================================================
1、knn算法的优点：
*思想简单，理论成熟，既可以用来做分类也可以用来做回归；
*可用于非线性分类；
*训练时间复杂度未O(n);
*准确度高，对数据没有假设，对outlier不敏感.
2、knn算法的缺点：
*计算量大
*样本不平衡问题（即有些类别的样本数量很多，而其他的样本的数量很少）
*需要大量的内存

##六、算法描述
k近邻算法简单、直观：给定一个训练数据集（包括类别标签），对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该输入实例分为这个类。
下面是knn的算法步骤：
a.令k是最近邻数目，D是训练样例的集合。
b.for每个测试样例，z=(x',y')do
c.   计算z和每个样例（x,y）<-D 之间的距离d(x',x)
d.   选择离z最近的k个训练样例集合Dz<- D 
e.   y'=argmax[(xi,yi)<-DzI(v=yi)
f.end for
对每个测试样例z=(x′,y′)，算法计算它和所有训练样例（x,y）属于D之间的距离（如欧氏距离，或相似度），以确定其最近邻列表Dz。
如果训练样例的数目很大，那么这种计算的开销就会很大。不过，可以使索引技术降低为测试样例找最近邻是的计算量。
特征空间中两个实例点的距离是两个实例相似程度的反映。
一旦得到最近邻列表，测试样例就可以根据最近邻的多数类进行分类，使用多数表决方法。

##七、K最邻近算法的实现（python）（KNN.py）
from numpy import *
import operator

class KNN:
    def createDataset(self):
        group = array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]])
        labels = ['A','A','B','B']
        return group,labels

    def KnnClassify(self,testX,trainX,labels,K):
        [N,M]=trainX.shape

    #calculate the distance between testX and other training samples
        difference = tile(testX,(N,1)) - trainX # tile for array and repeat for matrix in Python, == repmat in Matlab
        difference = difference ** 2 # take pow(difference,2)
        distance = difference.sum(1) # take the sum of difference from all dimensions
        distance = distance ** 0.5
        sortdiffidx = distance.argsort()

    # find the k nearest neighbours
        vote = {} #create the dictionary
        for i in range(K):
            ith_label = labels[sortdiffidx[i]];
            vote[ith_label] = vote.get(ith_label,0)+1 #get(ith_label,0) : if dictionary 'vote' exist key 'ith_label', return vote[ith_label]; else return 0
        sortedvote = sorted(vote.iteritems(),key = lambda x:x[1], reverse = True)
        # 'key = lambda x: x[1]' can be substituted by operator.itemgetter(1)
        return sortedvote[0][0]

k = KNN() #create KNN object
group,labels = k.createDataset()
cls = k.KnnClassify([0,0],group,labels,3)
print cls

