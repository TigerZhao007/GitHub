# 爬虫基础 python code

## 双色球最新30期数据
from bs4 import BeautifulSoup
import urllib.request
import numpy as np
import pandas as pd

url = "http://zst.aicai.com/ssq/openInfo/"
url_req = urllib.request.urlopen(url).read()
url_doc = url_req.decode('utf-8')

soup = BeautifulSoup(url_doc, 'html.parser')
soups = soup.find_all('td')

reds = []
for i in range(0,450):
    reds.append(soups[i].get_text())

# print(reds)
reds_re = np.array(reds).reshape(30,15)
reds_res = pd.DataFrame(reds_re, columns = {'x1','x2','x3','x4','x5','x6','x7','x8',
                              'x9','x10','x11','x12','x13','x14','x15'})
# print(reds_res)

## 链家最新1页数据（30个）
from bs4 import BeautifulSoup
import urllib.request
import numpy as np
import pandas as pd

url = 'https://nj.lianjia.com/chengjiao/pg'
headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36',
			}
resp = requests.get(url,headers).content
soup = BeautifulSoup(resp, 'lxml')
#print(soup)
house_list = soup.find_all('div',attrs={'class':'info'})
#print(house_list)
resp1 = requests.get(url).content
soup1 =BeautifulSoup(resp, 'lxml')
house_div = soup.find('div', attrs={'class':'content'})
lis = house_div.find_all('li')
values1 = []
for i in range(9,39):
    a = lis[i].find_all('div')[2].getText().split()
    values1.append(a)
print(pd.DataFrame(values1))

## 链家最新多页数据
from bs4 import BeautifulSoup
import urllib.request
import numpy as np
import pandas as pd

values = []
for j in range(1,3):
    url = 'https://nj.lianjia.com/chengjiao/pg'+ str(j) + '/'
    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36',
	     		}
    resp = requests.get(url,headers).content
    soup = BeautifulSoup(resp, 'lxml')
    # print(soup)
    house_list = soup.find_all('div',attrs={'class':'info'})
    # print(house_list)
    resp1 = requests.get(url).content
    soup1 =BeautifulSoup(resp, 'lxml')
    house_div = soup.find('div', attrs={'class':'content'})
    lis = house_div.find_all('li')
    values1 = []
    for i in range(9,39):
        a = lis[i].find_all('div')[2].getText().split()
        values1.append(a)
    values.extend(values1)
#print(values)
print(pd.DataFrame(values))

## 链家最新多页数据，及保存CSV数据
import requests
from bs4 import BeautifulSoup

def get_dat(url):   
   headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36',
	    		}
   resp = requests.get(url,headers).content
   soup = BeautifulSoup(resp, 'lxml')
   num  = soup.find_all('span',attrs={'class':'num'})
   lis_num = []
   for i in range(1,len(num),2):
       a = num[i].getText()
       lis_num.append(a)
   num1 = pd.DataFrame(lis_num)
   data  = soup.find_all('div',attrs={'class':'price-pre'})
   lis_data = []
   for i in range(len(data)):
       a = data[i].getText()
       lis_data.append(a)
   data1 = pd.DataFrame(lis_data)
   area  = soup.find_all('span',attrs={'class':'meters'})
   lis_area = []
   for i in range(len(area)):
       a = area[i].getText()
       lis_area.append(a)
   area1 = pd.DataFrame(lis_area)
   dat1 = pd.concat([data1,num1,area1],axis=1)
   dat1.columns = ['a','b','c']
   return dat1

url = 'https://nj.lianjia.com/zufang/pukou/pg'+ str(1) + 'rco10/'
dat = get_dat(url)
for i in range(2,38):
    url = 'https://nj.lianjia.com/zufang/pukou/pg'+ str(i) + 'rco10/'
    dat1 = get_dat(url)
    dat = pd.concat([dat,dat1])
#print(dat)
dat.to_csv('C:\\Users\\Administrator\\Desktop\\pukou38.csv') 

#url = 'https://nj.lianjia.com/zufang/gulou/pg'+ str(1) + 'rco10/' 51
#url = 'https://nj.lianjia.com/zufang/jianye/pg'+ str(1) + 'rco10/' 26
#url = 'https://nj.lianjia.com/zufang/qinhuai/pg'+ str(1) + 'rco10/' 46
#url = 'https://nj.lianjia.com/zufang/xuanwu/pg'+ str(1) + 'rco10/' 31
#url = 'https://nj.lianjia.com/zufang/yuhuatai/pg'+ str(1) + 'rco10/' 17
#url = 'https://nj.lianjia.com/zufang/qixia/pg'+ str(1) + 'rco10/' 27
#url = 'https://nj.lianjia.com/zufang/jiangning/pg'+ str(1) + 'rco10/' 67
#url = 'https://nj.lianjia.com/zufang/pukou/pg'+ str(1) + 'rco10/' 39
#url = 'https://nj.lianjia.com/zufang/liuhe/pg'+ str(1) + 'rco10/' 0
#url = 'https://nj.lianjia.com/zufang/lishui/pg'+ str(1) + 'rco10/' 0
#url = 'https://nj.lianjia.com/zufang/gaochun/pg'+ str(1) + 'rco10/' 0
#不同地区更换URL及页码数，分别进行爬取数据，得到各自的csv文件数据。


## 某某网站爬虫结果
import re
import requests
import pandas as pd
from bs4 import BeautifulSoup

url0 = 'http://www.73xi.com/html/part/31_2.html'
url1 = 'http://www.73xi.com/html/article/671543.html'
headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36',
       		}

def get_link(url,headers):   
   resp = requests.get(url,headers).content
   soup = BeautifulSoup(resp, 'lxml')
   num1 = soup.find_all('a')
   num2 = re.findall(r'<a href="/html/article/(.+?)" target="_blank"',str(num1))
   return num2

def get_txt(url,headers):   
   resp = requests.get(url,headers).content
   soup = BeautifulSoup(resp, 'lxml')
   num1 = soup.find_all('div',attrs={'class':'n_bd'})
#   num2 = re.findall(r'(.+?)<br/>',str(num1))
   return num1

dat0 = get_link(url0,headers)
f1 = open(r'C:\\Users\\Administrator\\Desktop\\73xi.txt','a',encoding='utf-8')
for link in dat0:
    print('正在爬取http://www.73xi.com/html/article/%s的文本'%(link))
    url1 = 'http://www.73xi.com/html/article/'+link
    dat1 = get_txt(url1,headers)
    f1.writelines(str(dat1))
    print('成功保存http://www.73xi.com/html/article/%s的文本'%(link))
f1.close()










