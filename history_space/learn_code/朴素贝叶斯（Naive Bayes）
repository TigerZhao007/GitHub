# 朴素贝叶斯

事件A和B同时发生的概率为在A发生的情况下发生B或者在B发生的情况下发生A
P(A∩B)=P(A)∗P(B|A)=P(B)∗P(A|B)
所以有：
P(A|B)=P(B|A)∗P(A)P(B)
对于给出的待分类项，求解在此项目出现的条件下各个目标类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。

工作原理
1.假设现在有样本x=（a1,a2,a3,...,an）这个待分类项（并认为x里面的特征独立）
2.再假设现在有分类目标Y={y1,y2,y3,...,yn}
3.那么max(P(y1|x),P(y2|x),...,P(yn|x))就是最终的分类类别
4.而P(yi|x)=P(x|yi）* P(yi)P(x)
5.因为x对于每个分类目标来说都一样，所以就是求max（P(x|yi)* P(yi)）
6.P(x|yi)* P(yi) = p(yi)* ∏i(P(ai|yi))
7.而具体的P(ai|yi)和P(yi)都是能从训练样本中统计出来
P(ai|yi)表示该类别下该特征出现的概率
P(yi)表示全部类别中这个类别出现的概率

工作流程
准备阶段: 确定特征属性，并对每个特征属性进行适当的划分，然后由人工对一部分待分类项进行分类，形成训练样本。
训练阶段: 计算每个类别在训练样本中的出现频率及每个特征属性划分对每个类别的条件概率估计
应用阶段: 使用分类器进行分类，输入是分类器和待分类样本，输出是样本属于的分类类别

#R语言代码（案例）
#!/usr/bin/Rscript
#构造训练集  
data <- matrix(c("sunny","hot","high","weak","no",  
                 "sunny","hot","high","strong","no",  
                 "overcast","hot","high","weak","yes",  
                 "rain","mild","high","weak","yes",  
                 "rain","cool","normal","weak","yes",  
                 "rain","cool","normal","strong","no",  
                 "overcast","cool","normal","strong","yes",  
                 "sunny","mild","high","weak","no",  
                 "sunny","cool","normal","weak","yes",  
                 "rain","mild","normal","weak","yes",  
                 "sunny","mild","normal","strong","yes",  
                 "overcast","mild","high","strong","yes",  
                 "overcast","hot","normal","weak","yes",  
                 "rain","mild","high","strong","no"), 
               byrow = TRUE,  
               dimnames = list(day = c(),condition = c("outlook","temperature","humidity","wind","playtennis")), 
               nrow=14, 
               ncol=5);  

#计算先验概率  
prior.yes = sum(data[,5] == "yes") / length(data[,5]);  
prior.no  = sum(data[,5] == "no")  / length(data[,5]);  

#贝叶斯模型  
naive.bayes.prediction <- function(condition.vec) {  
  # Calculate unnormlized posterior probability for playtennis = yes.  
  playtennis.yes <-  
    sum((data[,1] == condition.vec[1]) & (data[,5] == "yes")) / sum(data[,5] == "yes") * # P(outlook = f_1 | playtennis = yes)  
    sum((data[,2] == condition.vec[2]) & (data[,5] == "yes")) / sum(data[,5] == "yes") * # P(temperature = f_2 | playtennis = yes)  
    sum((data[,3] == condition.vec[3]) & (data[,5] == "yes")) / sum(data[,5] == "yes") * # P(humidity = f_3 | playtennis = yes)  
    sum((data[,4] == condition.vec[4]) & (data[,5] == "yes")) / sum(data[,5] == "yes") * # P(wind = f_4 | playtennis = yes)  
    prior.yes; # P(playtennis = yes)  
  
  # Calculate unnormlized posterior probability for playtennis = no.  
  playtennis.no <-  
    sum((data[,1] == condition.vec[1]) & (data[,5] == "no"))  / sum(data[,5] == "no")  * # P(outlook = f_1 | playtennis = no)  
    sum((data[,2] == condition.vec[2]) & (data[,5] == "no"))  / sum(data[,5] == "no")  * # P(temperature = f_2 | playtennis = no)  
    sum((data[,3] == condition.vec[3]) & (data[,5] == "no"))  / sum(data[,5] == "no")  * # P(humidity = f_3 | playtennis = no)  
    sum((data[,4] == condition.vec[4]) & (data[,5] == "no"))  / sum(data[,5] == "no")  * # P(wind = f_4 | playtennis = no)  
    prior.no; # P(playtennis = no)  
  
  return(list(post.pr.yes = playtennis.yes,  
              post.pr.no  = playtennis.no,  
              prediction  = ifelse(playtennis.yes >= playtennis.no, "yes", "no")));  
}  

#预测  
naive.bayes.prediction(c("overcast", "mild", "normal", "weak"));

