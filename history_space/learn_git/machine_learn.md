#机器学习-各分类模型优缺点

面试过程中经常被问到各种算法的优缺点，特此整理。<br>
https://www.jianshu.com/p/169dc01f0589<br>

## 1、决策树
#### 优点
一、 决策树`易于理解和解释`.人们在通过解释后都有能力去理解决策树所表达的意义。<br>
二、 对于决策树，数据的准备往往是简单或者是不必要的.其他的技术往往要求先把数据一般化，比如去掉多余的或者空白的属性。<br>
三、 能够`同时处理数据型和常规型属性`。其他的技术往往要求数据属性的单一。<br>
四、 决策树是一个`白盒模型`。如果给定一个观察的模型，那么根据所产生的决策树很容易推出相应的逻辑表达式。<br>
五、 易于通过静态测试来对模型进行评测。表示有可能测量该模型的可信度。<br>
六、 在相对短的时间内能够对大型数据源做出可行且效果良好的结果。<br>
七、 可以对有许多属性的数据集构造决策树。<br>
八、 决策树可很好地扩展到大型数据库中，同时它的大小独立于数据库的大小。<br>

#### 缺点
一、 对于那些各类别`样本数量不一致`的数据，在决策树当中,`信息增益的结果偏向于那些具有更多数值的特征`。<br>
二、 决策树处理`缺失数据`时的困难。<br>
三、 `过度拟合`问题的出现。<br>
四、 忽略数据集中`属性之间的相关性`。<br>

## 2、人工神经网络
#### 优点
分类的`准确度高`,并行`分布处理能力强`,`分布存储及学习能力强`，对噪声神经有`较强的鲁棒性和容错能力`，
能充分逼近复杂的非线性关系，具备`联想记忆`的功能等。<br>

#### 缺点
神经网络需要`大量的参数`，如网络拓扑结构、权值和阈值的初始值；不能观察之间的学习过程，输出结果难以解释，
会影响到结果的可信度和可接受程度；学习`时间过长`,甚至可能达不到学习的目的。<br>

## 3、KNN算法
#### 优点
一、 简单、有效。<br>
二、 重新训练的代价较低（类别体系的变化和训练集的变化，在Web环境和电子商务应用中是很常见的）。<br>
三、 计算时间和空间线性于训练集的规模（在一些场合不算太大）。<br>
四、 由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，
因此对于类域的`交叉或重叠较多`的待分样本集来说，KNN方法较其他方法更为适合。<br>
五、 该算法比较适用于`样本容量比较大`的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分。<br>

#### 缺点
一、 KNN算法是懒散学习方法（lazy learning,基本上不学习），一些积极学习的算法要快很多。<br>
二、 类别评分不是规格化的（不像概率评分）。<br>
三、 输出的`可解释性不强`，例如决策树的可解释性较强。<br>
四、 该算法在分类时有个主要的不足是，当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，
有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。该算法只计算“最近的”邻居样本，
某一类的样本数量很大，那么或者这类样本并不接近目标样本，或者这类样本很靠近目标样本。
无论怎样，数量并不能影响运行结果。可以采用权值的方法（和该样本距离小的邻居权值大）来改进。<br>
五、 计算量较大。目前常用的解决方法是事先对已知样本点进行剪辑，事先去除对分类作用不大的样本。<br>

## 4、支持向量机（SVM）
#### 优点
一、 可以解决`小样本`情况下的机器学习问题。<br>
二、 可以提高`泛化性能`。<br>
三、 可以解决`高维问题`。<br>
四、 可以解决`非线性问题`。<br>
五、 可以避免神经网络`结构选择`和`局部极小点`问题。<br>

#### 缺点
一、 对`缺失数据敏感`。<br>
二、 对非线性问题没有通用解决方案，必须谨慎选择Kernelfunction来处理。<br>

## 5、朴素贝叶斯
#### 优点
一、 朴素贝叶斯模型发源于古典数学理论，有着坚实的`数学基础`，以及稳定的`分类效率`。<br>
二、 NBC模型所需估计的参数很少，对缺失`数据不太敏感`，算法也比较简单。<br>

#### 缺点
一、 理论上，NBC模型与其他分类方法相比`具有最小的误差率`。但是`实际上并非总是如此`，这是因为NBC模型假设属性之间相互独立，
这个假设在实际应用中往往是不成立的（可以考虑用聚类算法先将相关性较大的属性聚类），这给NBC模型的正确分类带来了一定影响。
在属性个数比较多或者属性之间相关性较大时，NBC模型的分类效率比不上决策树模型。而在属性相关性较小时，NBC模型的性能最为良好。<br>
二、 需要知道`先验概率`。<br>
三、 分类决策存在错误率<br>

## 6、Adaboost算法
#### 优点
一、 adaboost是一种有很高精度的分类器。<br>
二、 可以使用各种方法构建子分类器，Adaboost算法提供的是框架。<br>
三、 当使用简单分类器时，计算出的结果是可以理解的。而且弱分类器构造极其简单。<br>
四、 简单，不用做特征筛选。<br>
五、 不用担心overfitting。<br>

#### 缺点
一、AdaBoost迭代次数也就是弱分类器数目不太好设定，可以使用交叉验证来进行确定。<br>
二、数据不平衡导致分类精度下降。<br>
三、训练比较耗时，每次重新选择当前分类器最好切分点。<br>

## 7、逻辑回归
#### 优点
一、预测结果是界于`0和1之间的概率`；<br>
二、可以适用于`连续性和类别性`自变量；<br>
三、容易使用和解释；<br>

#### 缺点
一、对模型中自变量`多重共线性较为敏感`，例如两个高度相关自变量同时放入模型，可能导致较弱的一个自变量回归符号不符合预期，
符号被扭转。?需要利用因子分析或者变量聚类分析等手段来选择代表性的自变量，以减少候选变量之间的相关性；<br>
二、预测结果`呈“S”型`，因此从log(odds)向概率转化的过程是非线性的，在两端随着?log(odds)值的变化，概率变化很小，
边际值太小，slope太小，而中间概率的变化很大，很敏感。 导致很多区间的变量变化对目标概率的影响没有区分度，无法确定阀值。<br>

## 8、随机森林
#### 优点
一、 在当前的很多数据集上，相对其他算法有着很大的优势，表现良好<br>
二、它能够处理很`高维度`（feature很多）的数据，并且不用做特征选择<br>
三、在训练完后，它能够给出`哪些feature比较重要`<br>
四、在创建随机森林的时候，对generlization error使用的是无偏估计，模型`泛化能力强`<br>
五、训练速度快，容易做成`并行化`方法<br>
六、在训练过程中，能够`检测到feature间的互相影响`<br>
七、实现比较简单<br>
八、对于`不平衡的数据集`来说，它可以平衡误差。<br>
九、如果有很大一部分的特征遗失，仍可以维持准确度。<br>

#### 缺点
一、随机森林已经被证明在某些`噪音较大`的分类或回归问题上`会过拟`<br>
二、对于有不同取值的属性的数据，取值划分较多的属性会对随机森林产生更大的影响，
所以随机森林在这种数据上产出的`属性权值是不可信的`。<br>

## 9、GBDT
#### 优点
一、可以灵活处理`各种类型`的数据，包括连续值和离散值。<br>
二、在相对少的调参时间情况下，预测的准备率也可以比较高。这个是相对SVM来说的。<br>
三、使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。<br>

#### 缺点
一、由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。<br>

## 参考原文<br>
1、http://bbs.pinggu.org/thread-2604496-1-1.html<br>
2、http://www.cnblogs.com/milkcoffeesugar/p/5769977.html<br>
3、http://blog.csdn.net/u012422446/article/details/53034260<br>
4、http://blog.sina.com.cn/s/blog_5dd0aaa50102vjq3.html<br>
5、http://blog.csdn.net/keepreder/article/details/47273297<br>
6、http://www.cnblogs.com/pinard/p/6140514.html<br>